import json
# =========================
# 1. åŠ è½½ JSON æ•°æ®
# =========================
with open("data/history_json/OmoT.json", "r", encoding="utf-8") as f:
    json_data = json.load(f)
    print(f"âœ… å·²åŠ è½½ JSON æ•°æ®ï¼Œå…± {len(json_data)} æ¡è®°å½•")

# =========================
# 2. LangChain è½¬æ¢ä¸º Document
# =========================
from langchain_core.documents import Document

documents = [
    Document(
        page_content=item["text"],
        metadata={
            "id": item["id"],
            "name": item["name"],
            "time": item["time"],
            "msgtype": item["msgtype"]
        }
    )
    for item in json_data
]

print(f"âœ… è½¬æ¢ä¸º Document å®Œæˆï¼Œå…± {len(documents)} æ¡è®°å½•")
print(documents[0].page_content, documents[0].metadata)
print("âœ… ä»¥ä¸Šä¸ºå‰10æ¡ã€‚")

# =========================
# 3. æ–‡æœ¬ Chunking
# =========================
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=10,   # é‡å å­—ç¬¦æ•°
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
)

chunks = splitter.split_documents(documents)
print(f"âœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼Œæ€»å…± {len(chunks)} ä¸ª chunk")
for chunk in chunks[0:10]:
    print(chunk.page_content, chunk.metadata)
    print(chunk,"\n")
print("âœ… ä»¥ä¸Šä¸ºå‰10ä¸ª chunkã€‚")

# =========================
# 4. Embedding
# =========================
from langchain_community.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="models/embedding/m3e-small")

# =========================
# 5. è½¬æ¢æˆå‘é‡
# =========================
# FAISS éœ€è¦çš„å°±æ˜¯ç›´æ¥æŠŠ Document åˆ—è¡¨ä¼ å…¥å‘é‡æ•°æ®åº“ï¼Œå®ƒä¼šè‡ªåŠ¨è®¡ç®— embedding
# ä½†å¦‚æœä½ æƒ³å•ç‹¬æ‹¿ vectorï¼Œä¹Ÿå¯ä»¥ï¼š
vectors = [embedding_model.embed_documents([chunk.page_content])[0] for chunk in chunks]

# =========================
# 6. å­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆFAISSï¼‰
# =========================
from langchain_community.vectorstores import FAISS

vector_db = FAISS.from_documents(chunks, embedding_model)

# ä¿å­˜åˆ°æœ¬åœ°
vector_db.save_local("data/chat_vector_db")
print("âœ… å·²ä¿å­˜å‘é‡æ•°æ®åº“åˆ° data/chat_vector_db")

# =========================
# 7. æµ‹è¯• top-k æ£€ç´¢
# =========================
query = "å¯çˆ±å¦¹å¦¹"
results = vector_db.similarity_search(query, k=20)
print(results,"\n")
print("ğŸ” top-k æ£€ç´¢ç»“æœï¼š")
for r in results:
    print(f"å†…å®¹ï¼š{r.page_content}")
    print(f"è¯´è¯è€…ï¼š{r.metadata['name']}, æ—¶é—´ï¼š{r.metadata['time']}")
    print("-" * 50)
